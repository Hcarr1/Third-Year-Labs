{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "pYWXqol3hCBG",
    "ExecuteTime": {
     "end_time": "2025-11-22T01:48:13.537900Z",
     "start_time": "2025-11-22T01:32:58.502752Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "num_epochs_scratch = 15\n",
    "learning_rate_scratch = 0.01\n",
    "\n",
    "# --- 1. Define the AlexNet Architecture (For Task 1) ---\n",
    "\n",
    "class AlexNetOriginal(nn.Module):\n",
    "    \"\"\"\n",
    "    AlexNet architecture using the original filter sizes and strides,\n",
    "    requiring 227x227 input images. Flattened size is 256 * 6 * 6 = 9216.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNetOriginal, self).__init__()\n",
    "\n",
    "        # --- Feature Extractor ---\n",
    "        self.features = nn.Sequential(\n",
    "            # Layer 1: Conv2d, ReLU, MaxPool2d\n",
    "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # Layer 2: Conv2d, ReLU, MaxPool2d\n",
    "            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # Layer 3: Conv2d, ReLU\n",
    "            nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Layer 4: Conv2d, ReLU\n",
    "            nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Layer 5: Conv2d, ReLU, MaxPool2d\n",
    "            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        FC_INPUT_SIZE = 256 * 6 * 6  # 9216\n",
    "\n",
    "        # --- Classifier ---\n",
    "        self.classifier = nn.Sequential(\n",
    "            # Layer 6: Linear , ReLU, Dropout\n",
    "            nn.Linear(in_features=FC_INPUT_SIZE, out_features=4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            # Layer 7: Linear , ReLU, Dropout\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            # Output Layer : Linear (to num_classes)\n",
    "            nn.Linear(in_features=4096, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through feature extractor\n",
    "        x = self.features(x)\n",
    "        # Flatten the output for the classifier\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass flattened features through classifier\n",
    "        final = self.classifier(x)\n",
    "        # Return the final output\n",
    "        return final\n",
    "\n",
    "# --- 2. Data Preparation (CIFAR-10) ---\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((227, 227)), # Resize for AlexNet\n",
    "    transforms.ToTensor(),\n",
    "    # Standard CIFAR-10 normalization\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "# Load and prepare data\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# --- General Training and Evaluation Functions ---\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, epochs, model_name=\"Model\"):\n",
    "    \"\"\"Generic training loop.\"\"\"\n",
    "    print(f\"\\n--- Starting Training for {model_name} ({epochs} epochs) ---\")\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 100 == 99:\n",
    "                print(f'[{model_name} - Epoch {epoch + 1}, Batch {i + 1}] Loss: {running_loss / 100:.3f}')\n",
    "                running_loss = 0.0\n",
    "    end_time = time.time()\n",
    "    print(f'Training Complete for {model_name}. Time taken: {end_time - start_time:.2f}s')\n",
    "\n",
    "def evaluate_model(model, test_loader, model_name=\"Model\"):\n",
    "    \"\"\"Generic evaluation function.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'\\nAccuracy of {model_name} on the test images: {accuracy:.2f} %')\n",
    "    return accuracy\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "#                         Task 1: Train from Scratch\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "print(\"=========================================================\")\n",
    "print(\"TASK 1: Training AlexNetOriginal from Scratch on CIFAR-10\")\n",
    "print(\"=========================================================\")\n",
    "\n",
    "model_scratch = AlexNetOriginal(num_classes=num_classes).to(device)\n",
    "criterion_scratch = nn.CrossEntropyLoss()\n",
    "optimizer_scratch = optim.SGD(model_scratch.parameters(),\n",
    "                              lr=learning_rate_scratch, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Train the model from scratch\n",
    "train_model(model_scratch, criterion_scratch, optimizer_scratch, trainloader, num_epochs_scratch, \"AlexNet Scratch\")\n",
    "accuracy_scratch = evaluate_model(model_scratch, testloader, \"AlexNet Scratch\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "=========================================================\n",
      "TASK 1: Training AlexNetOriginal from Scratch on CIFAR-10\n",
      "=========================================================\n",
      "\n",
      "--- Starting Training for AlexNet Scratch (15 epochs) ---\n",
      "[AlexNet Scratch - Epoch 1, Batch 100] Loss: 2.297\n",
      "[AlexNet Scratch - Epoch 1, Batch 200] Loss: 2.078\n",
      "[AlexNet Scratch - Epoch 1, Batch 300] Loss: 1.846\n",
      "[AlexNet Scratch - Epoch 2, Batch 100] Loss: 1.502\n",
      "[AlexNet Scratch - Epoch 2, Batch 200] Loss: 1.425\n",
      "[AlexNet Scratch - Epoch 2, Batch 300] Loss: 1.378\n",
      "[AlexNet Scratch - Epoch 3, Batch 100] Loss: 1.167\n",
      "[AlexNet Scratch - Epoch 3, Batch 200] Loss: 1.129\n",
      "[AlexNet Scratch - Epoch 3, Batch 300] Loss: 1.017\n",
      "[AlexNet Scratch - Epoch 4, Batch 100] Loss: 0.911\n",
      "[AlexNet Scratch - Epoch 4, Batch 200] Loss: 0.870\n",
      "[AlexNet Scratch - Epoch 4, Batch 300] Loss: 0.831\n",
      "[AlexNet Scratch - Epoch 5, Batch 100] Loss: 0.729\n",
      "[AlexNet Scratch - Epoch 5, Batch 200] Loss: 0.705\n",
      "[AlexNet Scratch - Epoch 5, Batch 300] Loss: 0.692\n",
      "[AlexNet Scratch - Epoch 6, Batch 100] Loss: 0.597\n",
      "[AlexNet Scratch - Epoch 6, Batch 200] Loss: 0.597\n",
      "[AlexNet Scratch - Epoch 6, Batch 300] Loss: 0.600\n",
      "[AlexNet Scratch - Epoch 7, Batch 100] Loss: 0.505\n",
      "[AlexNet Scratch - Epoch 7, Batch 200] Loss: 0.487\n",
      "[AlexNet Scratch - Epoch 7, Batch 300] Loss: 0.498\n",
      "[AlexNet Scratch - Epoch 8, Batch 100] Loss: 0.409\n",
      "[AlexNet Scratch - Epoch 8, Batch 200] Loss: 0.408\n",
      "[AlexNet Scratch - Epoch 8, Batch 300] Loss: 0.393\n",
      "[AlexNet Scratch - Epoch 9, Batch 100] Loss: 0.307\n",
      "[AlexNet Scratch - Epoch 9, Batch 200] Loss: 0.340\n",
      "[AlexNet Scratch - Epoch 9, Batch 300] Loss: 0.343\n",
      "[AlexNet Scratch - Epoch 10, Batch 100] Loss: 0.238\n",
      "[AlexNet Scratch - Epoch 10, Batch 200] Loss: 0.269\n",
      "[AlexNet Scratch - Epoch 10, Batch 300] Loss: 0.289\n",
      "[AlexNet Scratch - Epoch 11, Batch 100] Loss: 0.200\n",
      "[AlexNet Scratch - Epoch 11, Batch 200] Loss: 0.208\n",
      "[AlexNet Scratch - Epoch 11, Batch 300] Loss: 0.222\n",
      "[AlexNet Scratch - Epoch 12, Batch 100] Loss: 0.176\n",
      "[AlexNet Scratch - Epoch 12, Batch 200] Loss: 0.183\n",
      "[AlexNet Scratch - Epoch 12, Batch 300] Loss: 0.195\n",
      "[AlexNet Scratch - Epoch 13, Batch 100] Loss: 0.119\n",
      "[AlexNet Scratch - Epoch 13, Batch 200] Loss: 0.142\n",
      "[AlexNet Scratch - Epoch 13, Batch 300] Loss: 0.161\n",
      "[AlexNet Scratch - Epoch 14, Batch 100] Loss: 0.115\n",
      "[AlexNet Scratch - Epoch 14, Batch 200] Loss: 0.117\n",
      "[AlexNet Scratch - Epoch 14, Batch 300] Loss: 0.140\n",
      "[AlexNet Scratch - Epoch 15, Batch 100] Loss: 0.089\n",
      "[AlexNet Scratch - Epoch 15, Batch 200] Loss: 0.100\n",
      "[AlexNet Scratch - Epoch 15, Batch 300] Loss: 0.123\n",
      "Training Complete for AlexNet Scratch. Time taken: 900.55s\n",
      "\n",
      "Accuracy of AlexNet Scratch on the test images: 82.52 %\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below code can be used to get the total number of learnable parameters in your CNN model\n"
   ],
   "metadata": {
    "id": "ZZ3BU3rI8IoT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Count total number of parameters ---\n",
    "total_params = sum(p.numel() for p in model_scratch.parameters())\n",
    "trainable_params = sum(p.numel() for p in model_scratch.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters in AlexNet: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n"
   ],
   "metadata": {
    "id": "0I5szAUMwx4J",
    "ExecuteTime": {
     "end_time": "2025-11-22T01:48:13.569707Z",
     "start_time": "2025-11-22T01:48:13.565471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in AlexNet: 58,322,314\n",
      "Trainable parameters: 58,322,314\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ]
}
